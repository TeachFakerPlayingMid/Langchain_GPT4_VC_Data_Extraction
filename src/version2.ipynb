{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path() / \"../data\"\n",
    "#DATA_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/data')\n",
    "DATA_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def load_data(filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    return pd.read_csv(csv_path,encoding=encoding)\n",
    "\n",
    "def save_data(data, filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_csv(csv_path, index=False,encoding='ISO-8859-1')\n",
    "\n",
    "PLOT_PATH = Path() / \"../plot\"\n",
    "#PLOT_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/plot')\n",
    "PLOT_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300, transparent=True):\n",
    "    path = PLOT_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution, transparent=transparent)\n",
    "\n",
    "RESULT_PATH = Path() / \"../result\"\n",
    "#PLOT_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/plot')\n",
    "RESULT_PATH.mkdir(parents=True,exist_ok=True)\n",
    "def save_result(data, filename, data_path=RESULT_PATH):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def save_excel(data, filename, data_path=RESULT_PATH):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_excel(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object,Text, Number\n",
    "# Import the openai module\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def json_dump(json_object):\n",
    "    json_formatted_str = json.dumps(json_object, indent=2,ensure_ascii= False)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    role: Optional[str] = Field(\n",
    "        ..., description=\"The role or title of this person. \"\n",
    "                                )\n",
    "    company: Optional[str] = Field(\n",
    "        ..., description=\"The company this person is working in.\"\n",
    "    )\n",
    "    ai_related: Optional[bool] = Field(\n",
    "        ..., description=\"Whether this person is related to AI.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract\"\n",
    "            \"return null for the attribute's value.\"\n",
    "\n",
    "            \"The text we input is a person's biography. \"\n",
    "            \"You should treat every input text as a separate entity.\"\n",
    "\n",
    "            \"For each person's biography, you should consider:\"\n",
    "            \"If this person has several roles, we should only annotate one.\"\n",
    "            \"If this person is a AI company founder or co-founder, annotate him as a co-founder.\"\n",
    "            \"If this person is a researcher in AI feild, annotate this person as a researcher.\"\n",
    "            \"If you can't find any role, try to conclude the role from the context, if you still can't find any role, please annotate the role as NA.\"\n",
    "\n",
    "            \"For all the biographies with roles as NA, please read the context and try to judge if the biography is from a enterprise account's biography. \"\n",
    "            \"If it is, please annotate the role as 'Y'. If you can't judge, keep the role annotation as 'NA'.\"\n",
    "\n",
    "            \"when you extract company name, please extract the full name of the company and exclude any other punctuation mark such as @.\"\n",
    "            \"If there is no company name, please annotate the company name as NA.\"\n",
    "\n",
    "            \"If this person is related to AI, or anything about AI, please annotate the AI_related as true, otherwise annotate it as false.\"\n",
    "\n",
    "            \"The most important thing is you must extract exactly one combo of information for each input text! not more than one combo of information or less!\"\n",
    "\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List, TypedDict\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    # The name of the function right now corresponds\n",
    "                    # to the name of the pydantic model\n",
    "                    # This is implicit in the API right now,\n",
    "                    # and will be improved over time.\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\n",
    "        \"Research Scientist at Google DeepMind. I tweet about deep learning (research + software), music, generative models (personal account).\",\n",
    "        Person(role=\"Research Scientist\", company=\"Google DeepMind\", interest=\"deep learning, music, and generative models\", ai_related=True),\n",
    "    ),\n",
    "    (\n",
    "        \"I make videos.\\nSkill &gt; Destiny.\\nvi / vim\",\n",
    "        Person(role=None, company=None, interest=None, ai_related=False),\n",
    "    ),\n",
    "    (\n",
    "        \"ML Researcher, co-leading Superalignment @OpenAI. Optimizing for a post-AGI future where humanity flourishes.\",\n",
    "        Person(role=\"ML Researcher\", company=\"OpenAI\", interest=\"AGI\", ai_related=True),\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name = 'gpt-4-turbo-preview',\n",
    "    temperature = 0,\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "runnable = prompt | llm.with_structured_output(\n",
    "    schema=Person,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to remove illegal XML characters to every string in the DataFrame\n",
    "# and save directly to an Excel file without the intermediate cleaning step.\n",
    "def clean_and_save_dataset(dataframe):\n",
    "    # Define the function to remove illegal XML characters\n",
    "    def remove_illegal_xml_characters(s):\n",
    "        if isinstance(s, str):\n",
    "            return ''.join(char for char in s if ord(char) >= 32 or char in '\\t\\n\\r')\n",
    "        else:\n",
    "            return s\n",
    "    \n",
    "    # Apply the cleaning function across the DataFrame\n",
    "    cleaned_df = dataframe.applymap(remove_illegal_xml_characters)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter1.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "#df_new = df_new.sort_values(by='followersCount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_text = df_new['bio'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m bio_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_info\u001b[38;5;241m.\u001b[39mrole, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_info\u001b[38;5;241m.\u001b[39mcompany, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai_related\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_info\u001b[38;5;241m.\u001b[39mai_related}\n\u001b[1;32m----> 6\u001b[0m bio_df_new \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(bio_df_new\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[0;32m    659\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "df_intermediate = pd.DataFrame()\n",
    "for idx, text in enumerate(bio_text):\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    #bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_data = {\"role\": extracted_info.role, \"company\": extracted_info.company, \"ai_related\": extracted_info.ai_related,\"row_index\":idx}\n",
    "    bio_df_new = pd.DataFrame(bio_data, index=[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    print(bio_df_new.shape[0])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'NA', 'company': 'NA', 'ai_related': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_excel(result, 'result_temp1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter2.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter3.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    counter+=1\n",
    "    print(\"round: \" , counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter4.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" , counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "2\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "2\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n",
      "round:  0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "df_new = load_data('twitter5.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" , counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(role='NA', company='NA', ai_related=False)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter6.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter7.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp7.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter8.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp8.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter9.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp9.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter10.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp10.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter11.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])\n",
    "# Sort the DataFrame by 'tweetsCount' in descending order\n",
    "df_new = df_new.sort_values(by='followersCount', ascending=False)\n",
    "bio_text = df_new['bio'].tolist()\n",
    "df_intermediate = pd.DataFrame()\n",
    "counter = 0\n",
    "for text in bio_text:\n",
    "    extracted_info = runnable.invoke({\"text\": text, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    print(\"round: \" + counter)\n",
    "    print(bio_df_new.shape[0])\n",
    "    # extracted_info_list = pd.concat([extracted_info_list, bio_df_new])\n",
    "    df_intermediate = pd.concat([df_intermediate, bio_df_new])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "extracted_info_list = df_intermediate.reset_index(drop=True)  # Assuming it's a DataFrame\n",
    "result = pd.concat([df_new, extracted_info_list], axis=1)\n",
    "save_excel(result, 'result_temp11.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
