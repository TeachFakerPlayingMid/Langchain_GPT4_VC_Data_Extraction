{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path() / \"../data\"\n",
    "#DATA_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/data')\n",
    "DATA_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def load_data(filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    return pd.read_csv(csv_path,encoding=encoding)\n",
    "\n",
    "def save_data(data, filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_csv(csv_path, index=False,encoding='ISO-8859-1')\n",
    "\n",
    "PLOT_PATH = Path() / \"../plot\"\n",
    "#PLOT_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/plot')\n",
    "PLOT_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300, transparent=True):\n",
    "    path = PLOT_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution, transparent=transparent)\n",
    "\n",
    "RESULT_PATH = Path() / \"../result\"\n",
    "#PLOT_PATH = Path('/content/gdrive/My Drive/Colab Notebooks/VC_data_Cleaning_With_LLM/plot')\n",
    "RESULT_PATH.mkdir(parents=True,exist_ok=True)\n",
    "def save_result(data, filename, data_path=RESULT_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_csv(csv_path, index=False,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object,Text, Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def json_dump(json_object):\n",
    "    json_formatted_str = json.dumps(json_object, indent=2,ensure_ascii= False)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the openai module\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    role: Optional[str] = Field(\n",
    "        ..., description=\"The role or title of this person. \"\n",
    "                                )\n",
    "    company: Optional[str] = Field(\n",
    "        ..., description=\"The company this person is working in.\"\n",
    "    )\n",
    "    ai_related: Optional[bool] = Field(\n",
    "        ..., description=\"Whether this person is related to AI.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract\"\n",
    "            \"return null for the attribute's value.\"\n",
    "\n",
    "            \"The list we input is a list of people's biography. Each element in the list is a person's biography text.\"\n",
    "            \"You should treat every element in the list as a separate entity.\"\n",
    "\n",
    "            \"For each person, you should consider:\"\n",
    "            \"If this person has several roles, we should only annotate one.\"\n",
    "            \"If this person is a AI company founder or co-founder, annotate him as a co-founder.\"\n",
    "            \"If this person is a researcher in AI feild, annotate this person as a researcher.\"\n",
    "            \"If you can't find any role, try to conclude the role from the context, if you still can't find any role, please annotate the role as NA.\"\n",
    "\n",
    "            \"For all the biographies with roles as NA, please read the context and try to judge if the biography is from a enterprise account's biography. \"\n",
    "            \"If it is, please annotate the role as 'Y'. If you can't judge, keep the role annotation as 'NA'.\"\n",
    "\n",
    "            \"when you extract company name, please extract the full name of the company and exclude any other punctuation mark such as @.\"\n",
    "            \"If there is no company name, please annotate the company name as NA.\"\n",
    "\n",
    "            \"If this person is related to AI, or anything about AI, please annotate the AI_related as true, otherwise annotate it as false.\"\n",
    "\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List, TypedDict\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    # The name of the function right now corresponds\n",
    "                    # to the name of the pydantic model\n",
    "                    # This is implicit in the API right now,\n",
    "                    # and will be improved over time.\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\n",
    "        \"Research Scientist at Google DeepMind. I tweet about deep learning (research + software), music, generative models (personal account).\",\n",
    "        Person(role=\"Research Scientist\", company=\"Google DeepMind\", interest=\"deep learning, music, and generative models\", ai_related=True),\n",
    "    ),\n",
    "    (\n",
    "        \"I make videos.\\nSkill &gt; Destiny.\\nvi / vim\",\n",
    "        Person(role=None, company=None, interest=None, ai_related=False),\n",
    "    ),\n",
    "    (\n",
    "        \"ML Researcher, co-leading Superalignment @OpenAI. Optimizing for a post-AGI future where humanity flourishes.\",\n",
    "        Person(role=\"ML Researcher\", company=\"OpenAI\", interest=\"AGI\", ai_related=True),\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name = 'gpt-4-turbo-preview',\n",
    "    temperature = 0,\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TYS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "runnable = prompt | llm.with_structured_output(\n",
    "    schema=Data,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('twitter_combined.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df = df.dropna(subset=['bio'])\n",
    "\n",
    "# Combine all text in the 'bio' column into a list, considering only the first 100 texts\n",
    "documents = df['bio'].head(10).tolist()\n",
    "#documents = df['bio'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Making medicines differently to bring better drugs faster to the patients who benefit most with ML and data at scale.',\n",
       " '#LearnWithoutLimits on Coursera. Access online courses and degrees from world-class universities and companies. Visit https://t.co/iZWWGG1ypn for support.',\n",
       " 'The leading news source for higher education. Get our newsletters: https://t.co/zfAnZfGsHc',\n",
       " \"Sharing things I'm learning through my foundation work and other interests.\",\n",
       " 'Policy research at @openai. I mostly tweet about AI, animals, and sci-fi. He/him. Views my own.',\n",
       " 'Co-Founder of OpenAI',\n",
       " 'Chief Scientist, Google DeepMind and Google Research. Co-designer/implementor of things like @TensorFlow, MapReduce, Bigtable, Spanner, Gemini .. (he/him)',\n",
       " 'VP of Research & Deep Learning Lead, Google DeepMind. Gemini co-lead.\\n\\nPast: AlphaStar, AlphaFold, AlphaCode, WaveNet, seq2seq, distillation, TF.',\n",
       " '@AnthropicAI, ONEAI OECD, co-chair @indexingai, writer @ https://t.co/3vmtHYkaTu Past: @openai, @business @theregister. Neural nets, distributed systems, weird futures',\n",
       " 'Research Scientist at Google DeepMind. I tweet about deep learning (research + software), music, generative models (personal account).']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "  Policy research at @openai. \n",
    "  I mostly tweet about AI, animals, and sci-fi. He/him. Views my own.\n",
    "  Chief Scientist, Google DeepMind and Google Research. Co-designer/implementor of things like @TensorFlow, MapReduce, Bigtable, Spanner, Gemini .. (he/him)\n",
    "'''\n",
    "texts = ['Making medicines differently to bring better drugs faster to the patients who benefit most with ML and data at scale.',\n",
    " '#LearnWithoutLimits on Coursera. Access online courses and degrees from world-class universities and companies. Visit https://t.co/iZWWGG1ypn for support.',\n",
    " 'The leading news source for higher education. Get our newsletters: https://t.co/zfAnZfGsHc']\n",
    "\n",
    "text = documents\n",
    "\n",
    "output = runnable.invoke({\"text\": text, \"examples\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(role='NA', company='NA', ai_related=True), Person(role='NA', company='Coursera', ai_related=False), Person(role='NA', company='NA', ai_related=False), Person(role='NA', company='NA', ai_related=False), Person(role='researcher', company='OpenAI', ai_related=True), Person(role='co-founder', company='OpenAI', ai_related=True), Person(role='researcher', company='Google DeepMind', ai_related=True), Person(role='researcher', company='Google DeepMind', ai_related=True), Person(role='co-founder', company='AnthropicAI', ai_related=True), Person(role='researcher', company='Google DeepMind', ai_related=True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>ai_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA</td>\n",
       "      <td>Coursera</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>researcher</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>co-founder</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>researcher</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>researcher</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>co-founder</td>\n",
       "      <td>AnthropicAI</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>researcher</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         role          company  ai_related\n",
       "0          NA               NA        True\n",
       "1          NA         Coursera       False\n",
       "2          NA               NA       False\n",
       "3          NA               NA       False\n",
       "4  researcher           OpenAI        True\n",
       "5  co-founder           OpenAI        True\n",
       "6  researcher  Google DeepMind        True\n",
       "7  researcher  Google DeepMind        True\n",
       "8  co-founder      AnthropicAI        True\n",
       "9  researcher  Google DeepMind        True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract information to a list of dictionaries\n",
    "people_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in output.people]\n",
    "\n",
    "# Convert to DataFrame\n",
    "bio_df = pd.DataFrame(people_data)\n",
    "\n",
    "bio_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = load_data('twitter_combined.csv')\n",
    "# Drop NaN values from the 'bio' column\n",
    "df_new = df_new.dropna(subset=['bio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 30\n",
    "chunks = [df_new.iloc[i:i + chunk_size] for i in range(0, df_new.shape[0], chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each chunk and store the results\n",
    "extracted_info_list = []\n",
    "for chunk in chunks:\n",
    "    chunk_text = chunk['bio'].tolist()\n",
    "    extracted_info =  runnable.invoke({\"text\": chunk, \"examples\": messages})\n",
    "    bio_data = [{\"role\": person.role, \"company\": person.company, \"ai_related\": person.ai_related} for person in extracted_info.people]\n",
    "    bio_df_new = pd.DataFrame(bio_data)\n",
    "    extracted_info_list.append(bio_df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the extracted information from all chunks\n",
    "extracted_info_combined = pd.concat(extracted_info_list).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the order of rows hasn't changed, directly add the extracted info to the DataFrame\n",
    "df_new.loc[:, 'role'] = extracted_info_combined['role']\n",
    "df_new.loc[:, 'company'] = extracted_info_combined['company']\n",
    "df_new.loc[:, 'ai_related'] = extracted_info_combined['ai_related']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
